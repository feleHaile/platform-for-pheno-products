{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.input_file_name\n",
    "import scala.sys.process.Process\n",
    "import sys.process._\n",
    "import java.io.File;\n",
    "import scala.Byte\n",
    "import scala.util.Random\n",
    "import java.nio.file.{Files, Paths}\n",
    "import java.nio.ByteBuffer\n",
    "import java.util.Arrays \n",
    "import java.nio.ByteOrder\n",
    "import java.io.RandomAccessFile\n",
    "import java.io.PrintWriter\n",
    "import java.io.FileWriter\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.lang.Math.toIntExact\n",
    "import scala.reflect.ClassTag\n",
    "import sys.process._\n",
    "import geotrellis.raster._\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.input.PortableDataStream\n",
    "import scala.collection.immutable.Map\n",
    "import org.apache.commons.io.FilenameUtils;\n",
    "import org.apache.spark.sql.functions.input_file_name\n",
    "import scala.sys.process.Process\n",
    "import sys.process._\n",
    "import scala.Byte\n",
    "import java.nio.file.{Files, Paths}\n",
    "import java.nio.ByteBuffer\n",
    "import java.util.Arrays \n",
    "import java.nio.ByteOrder\n",
    "import java.lang.Math.toIntExact\n",
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "import org.apache.spark.HashPartitioner\n",
    "import java.util.Calendar\n",
    "import java.time.temporal.ChronoUnit\n",
    "import org.apache.spark.Partitioner\n",
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//CONTROL PARAMETERS (NDVI and EVI)\n",
    "//Tilling \n",
    "var startRow = 1 \n",
    "var endRow = 3000\n",
    "var starColumn = 1\n",
    "var endColumn = 6500\n",
    "var numberTilesRow = 32\n",
    "var numberTilesColumn = 32\n",
    "var number_tiles = numberTilesRow * numberTilesColumn\n",
    "\n",
    "//Settings files  parameters\n",
    "var n_year = \"19\"\n",
    "var n_points_per_year = \"36\"\n",
    "val settigns_dir = \"/data/local/home/parrot/minio2/settigs_dir/\"\n",
    "var settingsNameTemplate = \"template.set\"\n",
    "val outputSettingName = \"USA_part\"\n",
    "val settingsList: String = \"/data/local/home/parrot/minio2/listfile_cluster.txt\"\n",
    "val executables_dir =  \"/data/local/home/parrot/minio2/executables/\"\n",
    "val executable_path = executables_dir + \"TSF_process.x64\"\n",
    "\n",
    "var filepath = \"hdfs:///user/hadoop/evi_new_template.tif\"\n",
    "\n",
    "//Change here parameters for outpt paths\n",
    "var results_folder = \"performance_testing/\"\n",
    "var results_folderSOS = \"performance_testing/\"\n",
    "\n",
    "var save_path = \"/data/local/home/parrot/minio2/\" + results_folder\n",
    "var save_path_sos = \"/data/local/home/parrot/minio2/\" + results_folderSOS\n",
    "var save_path_hdfs = \"hdfs:///user/spark/\" + results_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Settigns Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateSettigsFilesForEachNode() : Unit = {\n",
    "    \n",
    "    val rowRange = endRow - startRow\n",
    "    var rowCounter = startRow\n",
    "    \n",
    "    var columnRange = endColumn - starColumn\n",
    "    var columnCounter = starColumn\n",
    "\n",
    "    var settingFile = new File(\"/data/local/home/parrot/minio2/\" + settingsNameTemplate) \n",
    "//     val settingFile = new File(\"/data/local/home/parrot/minio2/\" + settingsNameTemplate) \n",
    "    val fw_list = new FileWriter(settingsList)\n",
    "\n",
    "    val lines = scala.io.Source.fromFile(settingFile).mkString.split(\"\\\\r?\\\\n\")\n",
    "    var endRowInc =startRow\n",
    "    var endColumnInc =starColumn\n",
    "    var tile_number = 1\n",
    "    \n",
    "    \n",
    "    for( rowTile <- 1 to numberTilesRow){\n",
    "      if( rowTile != numberTilesRow) {\n",
    "        endRowInc = rowCounter + rowRange / numberTilesRow\n",
    "      }\n",
    "      else{\n",
    "        // last task, compensate for the pixel increment and non-whole division\n",
    "        endRowInc = (rowCounter + rowRange  / numberTilesRow) - numberTilesRow + 1 + (rowRange % numberTilesRow)\n",
    "      }\n",
    "        \n",
    "      for( columnTile <- 1 to numberTilesColumn){\n",
    "          if( columnTile != numberTilesColumn) {\n",
    "            endColumnInc = columnCounter + columnRange / numberTilesColumn\n",
    "          }\n",
    "          else{\n",
    "            // last task, compensate for the pixel increment and non-whole division\n",
    "            endColumnInc = (columnCounter + columnRange  / numberTilesColumn) - numberTilesColumn + 1 + (columnRange % numberTilesColumn)\n",
    "          }\n",
    "        \n",
    "          //change corresponding lines         \n",
    "          lines(1)= \"cluster\" + tile_number +\"      %Job_name (no blanks)\" //change job name\n",
    "          lines(10) = rowCounter.toString + \" \"+ endRowInc.toString + \" \"+ columnCounter.toString+\" \"+ endColumnInc.toString +\"      %Processing window (start row end row start col end col)\"\n",
    "          lines(11) = n_year + \" \" + n_points_per_year  + \"           %No. years and no. points per year\"\n",
    "          columnCounter +=  columnRange /numberTilesColumn +1\n",
    "\n",
    "          println( \"Settings file: \" + tile_number )\n",
    "          println(lines(10))\n",
    "\n",
    "          val settingFileOut = new File(settigns_dir + outputSettingName+\"_cluster\" + \".set\"+ tile_number) // Temporary File\n",
    "          new PrintWriter(settingFileOut){write(lines.mkString(\"\\n\"))}.close() //close file\n",
    "          fw_list.write(settigns_dir + outputSettingName+\"_cluster\" + \".set\"+ tile_number + \"\\t\" +\"cluster\" + tile_number +\"_TS.tpa\" + \"\\t\" +\"cluster\" + tile_number +\"_TS.ndx\"  + \"\\n\")\n",
    "          tile_number = tile_number + 1\n",
    "      }\n",
    "        rowCounter +=  rowRange /numberTilesRow +1\n",
    "        columnCounter = starColumn\n",
    "        endColumnInc =starColumn\n",
    "    }\n",
    "     fw_list.close() \n",
    "}\n",
    "    \n",
    "CreateSettigsFilesForEachNode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Jobs in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactPartitioner(override val numPartitions: Int) extends Partitioner {\n",
    "\n",
    "  def getPartition(key: Any): Int = key match {\n",
    "    // `k` is assumed to go continuously from 0 to elements-1.\n",
    "    case _ =>      key.asInstanceOf[Int] \n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//Argumenets: settings path, output .tpa file\n",
    "var settings_rdd : RDD[(String, String,String)] = sc.textFile(\"file://\" + settingsList)\n",
    ".map(str => str.split(\"\\t\"))\n",
    ".map(arr => (arr(0), arr(1), arr(2)))\n",
    "\n",
    "// Repartition on each line \n",
    "//default partitioner can assign several rdd records (block_ids) to one partitioner, few empty\n",
    "var settings_rdd_part = settings_rdd.repartition(number_tiles).zipWithIndex.map{case (k,v) => (v.toInt,k)} \n",
    "\n",
    "var one_task_per_record : RDD[(String,String,String)]= settings_rdd_part.partitionBy(new ExactPartitioner(number_tiles)).values\n",
    "\n",
    "\n",
    "one_task_per_record.mapPartitionsWithIndex((idx, it) => Iterator((idx, it.toList))).toDF(\"partition_id\", \"block_ids\").show(2024,false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val res = one_task_per_record.map( setting_tile => { Seq(executable_path, setting_tile._1,\"1\" ).!\n",
    "                               Seq(\"mv\",setting_tile._2,save_path).!\n",
    "                               Seq(\"mv\",setting_tile._3,save_path).!})\n",
    "res.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Run if Execution Problems, Will stop the TimeSat process from within the machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var kill = one_task_per_record.map( setting_tile => { Seq(\"sudo\",\"pkill\",\"TSF_process.x64\").!})\n",
    "kill.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//Execute to set the permissions: HADOOP_USER_NAME=root hadoop fs -chmod -R 777 /user/spark\n",
    "val cmd = \"hadoop fs -copyFromLocal \" + save_path + \" \" + \"/user/spark\"\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var data = sc.textFile(\"file:///data/local/home/parrot/minio2/listfileNdviNew.txt\")\n",
    "var count  = names.first()\n",
    "var filtered_dates = data.filter(row => row != count)\n",
    "var new2 = filtered_dates.map( data => data.split(\"\\\\.\")).map(path => path(0).split(\"/\")).map(path => path(path.size -1))\n",
    "\n",
    "var new3 =new2.zipWithIndex().map { case (v, i) => (i+1, v) } //cache\n",
    "var index_date  = new3.collectAsMap().map(kv => (kv._1,kv._2)).toMap\n",
    "\n",
    "var index_dateBC = sc.broadcast(index_date)\n",
    "\n",
    "index_date.get(28)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Binary Input into rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var rdd_tpa = sc.binaryFiles(save_path_hdfs + \"/*.tpa\").map(m => (FilenameUtils.getBaseName(m._1).split(\"_\")(0), m._2))\n",
    "var rdd_tpx = sc.binaryFiles(save_path_hdfs+ \"/*.ndx\").map(m => (FilenameUtils.getBaseName(m._1).split(\"_\")(0), m._2))\n",
    "\n",
    "var all :RDD[(String,(PortableDataStream,PortableDataStream))]  = rdd_tpa.join(rdd_tpx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Seasonality Information per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getSeasons(filename: String, indexStream: PortableDataStream, seasonalityStream: PortableDataStream  ): Array[Float] = {\n",
    "    var index : Array[Byte] = indexStream.toArray()\n",
    "    if(index.length != 0){\n",
    "        var seasonality : Array[Byte] = seasonalityStream.toArray()\n",
    "    //     var index_tuple : Array[Long] = new Array[Long]( index.length/8)\n",
    "        var nseasons_int_def = toIntExact(ByteBuffer.wrap(index.slice(16,24)).order(ByteOrder.LITTLE_ENDIAN).getLong)\n",
    "        var array = new Array[Float](index.length/32 * nseasons_int_def *16)\n",
    "        var arrayIndex = 0\n",
    "\n",
    "        //loop on the pixel positions \n",
    "        for(i <- 0 until index.length by 32){ \n",
    "\n",
    "            var nseasons_int = toIntExact(ByteBuffer.wrap(index.slice(i+16,i+24)).order(ByteOrder.LITTLE_ENDIAN).getLong)   // number seasons\n",
    "            var position_int = toIntExact(ByteBuffer.wrap(index.slice(i+24,i+32)).order(ByteOrder.LITTLE_ENDIAN).getLong) -1 //adjust for somewhat -1 offset in the file \n",
    "\n",
    "            //loop on the available seasons \n",
    "            for (z <- 1 to nseasons_int){\n",
    "                array(arrayIndex) = ByteBuffer.wrap(index.slice(i,i+8)).order(ByteOrder.LITTLE_ENDIAN).getLong //row\n",
    "                array(arrayIndex + 1) = ByteBuffer.wrap(index.slice(i+8,i+16)).order(ByteOrder.LITTLE_ENDIAN).getLong //column\n",
    "                array(arrayIndex + 2) = z //number of season we are reading\n",
    "                if (z.equals(1)) position_int+= 12  //skip row,column, number seasons, we have them from index file\n",
    "                //loap the  seasonality parametars\n",
    "                for (m <- 1 to 13){ \n",
    "                    array(arrayIndex + 2 + m) = (ByteBuffer.wrap(seasonality.slice(position_int, position_int+4)).order(ByteOrder.LITTLE_ENDIAN).getFloat)\n",
    "                    position_int+= 4\n",
    "                }\n",
    "                arrayIndex = arrayIndex + 3 + 13\n",
    "            }\n",
    "        }\n",
    "        return array\n",
    "      }\n",
    "    else{\n",
    "        return new Array[Float](16).fill(-1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "var all_seasonality_parametars  = all.flatMap(m => { getSeasons(m._1, m._2._2, m._2._1).grouped(16).toList}).filter(m => m(0) != -1)\n",
    "\n",
    "//here we drop the othe seasonality parameteers and get SOS only \n",
    "//tuple in the form (x,y,season,SOS) --> [(1609.0,2721.0,1.0,45.88774), (1609.0,2721.0,2.0,80.46434), ...]\n",
    "var sos_tuple = all_seasonality_parametars.map(m => (m(0),m(1),m(2),m(3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var sos_tuple_s1  = sos_tuple.filter(x => x._3 == 15.toFloat).filter(x => x._1 == 1000).map{case (x,y,season,value) => {\n",
    "       \n",
    "        (x,y,season,value)\n",
    "    }}\n",
    "\n",
    "sos_tuple_s1.take(1000).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//find number of seasons //think of a better way, instead of going throug all the rdd\n",
    "val n_seasons_found : Int = sos_tuple.map(x => x._3).max().toInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Single band GeoTiff\n",
    "//Since it is a single GeoTiff, it will be a RDD with a tile.\n",
    "var geoTiffs : RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(filepath)\n",
    "var proExtents_RDD : RDD[ProjectedExtent] = geoTiffs.keys\n",
    "var tiles_RDD : RDD[Tile]= geoTiffs.values\n",
    "\n",
    "\n",
    "var grids_RDD = tiles_RDD.map(m => m.toArrayDouble())\n",
    "\n",
    "var bands_withIndex = grids_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "\n",
    "//Array[(Long, Double)]\n",
    "var grid0 = bands_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}    //\n",
    "\n",
    "var extents_withIndex = proExtents_RDD.zipWithIndex().map{case (e,v) => (v,e)} //cache\n",
    "var projected_extent = (extents_withIndex.filter(m => m._1 == 0).values.collect())(0) //.cache.collect())(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var filtered = sos_tuple.filter(x => x._3 == 1.0 && x._4 > 27).map{ case (x, y,season,value) => (x, y) }\n",
    "\n",
    "//NDVI\n",
    "var filtered2 = sos_tuple.filter(x => x._3 == 18.0 && (x._4 > 639 || x._4 == 0)).map{ case (x, y,season,value) => (x, y) }\n",
    "var intersection = sc.broadcast(filtered.intersection(filtered2).collect.toSet)\n",
    "\n",
    "var aligned_seasons_1 = sos_tuple.map{ case (x, y,season,value) => {\n",
    "    if (intersection.value.contains((x,y))){\n",
    "        (x, y,season + 1,value)\n",
    "    }else{\n",
    "    (x, y,season,value)\n",
    "    }\n",
    "    \n",
    "}} //cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate GeoTiff for each available season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "var num_cols :Int = 6500\n",
    "var num_rows :Int = 3000\n",
    "var num_colsB = sc.broadcast(num_cols)\n",
    "var start_year = 1999\n",
    "var start_yearB = sc.broadcast(start_year)\n",
    "var season_number = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for( season_number <- 1 to n_seasons_found + 1){\n",
    "     \n",
    "    var sos_tuple_s1  = aligned_seasons_1.filter(x => x._3 == season_number.toFloat).filter(x => x._4 != 0.0).map{case (x,y,season,value) => {\n",
    "        var rounded = Math.rint( (value -1)*10.139)   //round to int by using 2 digits of precision\n",
    "        var date_TimeSat : Calendar = Calendar.getInstance()\n",
    "        \n",
    "        var leap_year_days = 0\n",
    "        if(season_number >= 2 && season_number<=5){\n",
    "            leap_year_days = 1\n",
    "        }\n",
    "        else if (season_number >= 6 && season_number<= 9){\n",
    "             leap_year_days = 2\n",
    "        }\n",
    "        else if (season_number >= 10 && season_number<= 13){\n",
    "             leap_year_days = 3\n",
    "        }\n",
    "        else if (season_number >= 14 && season_number<= 17){\n",
    "             leap_year_days = 4\n",
    "        }else if (season_number> 17){\n",
    "            leap_year_days = 5\n",
    "        }   \n",
    "        \n",
    "        date_TimeSat.set(start_yearB.value , 0, 3)\n",
    "        date_TimeSat.add(Calendar.DATE, rounded.toInt + leap_year_days)\n",
    "        var day_of_year = date_TimeSat.get(Calendar.DAY_OF_YEAR)\n",
    "        //offster early spings\n",
    "        if(day_of_year < 270){\n",
    "            (x,y,season,day_of_year)}\n",
    "        else{\n",
    "            (x,y,season,day_of_year- 365)}\n",
    "        \n",
    "    \n",
    "    }}\n",
    "    \n",
    "    print(\"=====Calculating Season \"+ season_number)\n",
    "//      sos_tuple_s2.take(1000).foreach(println)\n",
    "\n",
    "    //The .get(col,row) methods are implemented by using the equation cols * row + col to translate from grid coordinates to array index\n",
    "    var season_to_array = sos_tuple_s1.map(x => (num_colsB.value.toInt*(x._1.toInt -1) + (x._2.toInt- 1).toLong, x._4))\n",
    "\n",
    "    var result_2: RDD[(Long, (Double, Option[Int]))] = grid0.leftOuterJoin(season_to_array)\n",
    "    // result_2.take(20).foreach(println)\n",
    "\n",
    "    //TODO: 1. If value in original .tiff  , but None in seasonality ==> set NaN for the corresponding index\n",
    "    var result_3: RDD[(Long, Double)] = result_2.map { \n",
    "      case (index, (value_org, value_pheno)) => {if (value_pheno.isDefined)  (index,value_pheno.get)\n",
    "                                                 else (index, Double.NaN)  }\n",
    "    }\n",
    "    \n",
    "\n",
    "    var season = DoubleArrayTile(result_3.sortBy(_._1).map(_._2).collect() , num_cols, num_rows)\n",
    "\n",
    "\n",
    "    var geoTif = new SinglebandGeoTiff(season, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "    var year = start_year + season_number -1\n",
    "    var output = save_path_sos + year + \".tif\"\n",
    "    GeoTiffWriter.write(geoTif, output)    \n",
    " }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
