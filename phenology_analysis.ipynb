{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import geotrellis.raster.io.geotiff._\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{GeoTiff, SinglebandGeoTiff}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile}\n",
    "import geotrellis.raster._\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.util.StatCounter\n",
    "import scala.sys.process.Process\n",
    "import sys.process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cmd = hadoop fs -copyFromLocal /data/local/home/parrot/minio2/EVI_experiment_1_SOS /user/hadoop\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cmd = \"hadoop fs -copyFromLocal \" + \"/data/local/home/parrot/minio2/EVI_experiment_1_SOS\" + \" /user/hadoop\"\n",
    "Process(cmd)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pattern = tif\n",
       "filepath = hdfs:///user/hadoop/EVI_experiment_1_SOS\n",
       "tiles_RDD = MapPartitionsRDD[2] at values at <console>:63\n",
       "grids_RDD = EmptyRDD[3] at emptyRDD at <console>:64\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EmptyRDD[3] at emptyRDD at <console>:64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "val pattern: String = \"tif\"\n",
    "var filepath: String = \"hdfs:///user/hadoop/EVI_experiment_1_SOS\"\n",
    "val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values\n",
    "var grids_RDD :RDD[Array[Double]] = sc.emptyRDD\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group the pixels from all images  on pixel positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grids_RDD = MapPartitionsRDD[4] at map at <console>:66\n",
       "indexedValues = MapPartitionsRDD[5] at map at <console>:68\n",
       "flattenValues = MapPartitionsRDD[7] at filter at <console>:71\n",
       "groupedValues = ShuffledRDD[8] at groupByKey at <console>:73\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[8] at groupByKey at <console>:73"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "\n",
    "var grids_RDD: RDD[Array[Double]] = tiles_RDD.map(m => m.toArrayDouble())\n",
    "\n",
    "var indexedValues: RDD[Array[(Int, Double)]] =  grids_RDD.map(m => m.zipWithIndex.map{case (e,v) => (v,e)})\n",
    "indexedValues.partitions\n",
    "\n",
    "var flattenValues : RDD[(Int, Double)] = indexedValues.flatMap(m => m).filter( m => !m._2.isNaN)\n",
    "\n",
    "var groupedValues : RDD[(Int, Iterable[Double])] = flattenValues.groupByKey(1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "averagedValues = MapPartitionsRDD[10] at map at <console>:71\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[10] at map at <console>:71"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "val averagedValues = groupedValues.mapValues(value =>\n",
    "StatCounter(value).mean).map(m => (m._1.toLong, Math.rint(m._2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdevduration = MapPartitionsRDD[12] at map at <console>:71\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at map at <console>:71"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stdevduration = groupedValues.mapValues(value =>\n",
    "StatCounter(value).stdev).map(m => (m._1.toLong, Math.rint(m._2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_cols_rows = (6500,3000)\n",
       "tile0 = DoubleConstantNoDataArrayTile([D@5f35aba,6500,3000)\n",
       "num_cols_rows = (6500,3000)\n",
       "filepath2 = hdfs:///user/hadoop/evi_new_template.tif\n",
       "geoTiff = MapPartitionsRDD[14] at mapPartitions at HadoopGeoTiffRDD.scala:109\n",
       "tile_my = MapPartitionsRDD[15] at values at <console>:78\n",
       "proExtents_RDD = MapPartitionsRDD[16] at keys at <console>:81\n",
       "extents_withIndex = MapPartitionsRDD[18] at map at <console>:82\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "projected_extent: geotrellis.vector.Projecte...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[18] at map at <console>:82"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "var num_cols_rows :(Int, Int) = (0, 0)\n",
    "\n",
    "var tile0 = (tiles_RDD.take(1))(0)\n",
    "num_cols_rows = (tile0.cols,tile0.rows)\n",
    "\n",
    "var filepath2 = \"hdfs:///user/hadoop/evi_new_template.tif\"\n",
    "//EVI\n",
    "// var filepath = \"hdfs:///user/hadoop/evi_template.tif\"\n",
    "//Since it is a single GeoTiff, it will be a RDD with a tile.\n",
    "var geoTiff : RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(filepath2)\n",
    "// var proExtents_RDD : RDD[ProjectedExtent] = geoTiff.keys\n",
    "var tile_my : RDD[Tile]= geoTiff.values\n",
    "\n",
    "\n",
    "var proExtents_RDD : RDD[ProjectedExtent] = geoTiff.keys\n",
    "var extents_withIndex = proExtents_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "var projected_extent = (extents_withIndex.filter(m => m._1 == 0).values.cache.collect())(0)\n",
    "\n",
    "\n",
    "var grids_RDD = tile_my.map(m => m.toArrayDouble())\n",
    "var bands_withIndex = grids_RDD.zipWithIndex().map { case (v, i) => (i, v) }\n",
    "var grid0 : RDD[(Long, Double)] = bands_withIndex.filter(m => m._1 == 0).values.flatMap( m => m).zipWithIndex.map{case (v,i) => (i,v)}.cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce GeoTiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 19:==================================================> (973 + 17) / 1000]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result_2 = MapPartitionsRDD[41] at leftOuterJoin at <console>:90\n",
       "result = MapPartitionsRDD[42] at map at <console>:94\n",
       "season = DoubleConstantNoDataArrayTile([D@d53c0cb,6500,3000)\n",
       "geoTif = SinglebandGeoTiff(DoubleConstantNoDataArrayTile([D@d53c0cb,6500,3000),Extent(-124.995535714211, 23.209821428584302, -66.95982142841754, 49.9955357142925),geotrellis.proj4.CRS$$anon$3@41d0d1b7,Tags(Map(),List()),GeoTiffOptions(geotrellis.raster.io.geotiff.Striped@21ad1100,geotrellis.raster.io.geotiff.compression.DeflateCompression$@1354f44,1,None))\n",
       "output = /data/local/home/parrot/minio2/average_1.tif\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/data/local/home/parrot/minio2/average_1.tif"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " var result_2: RDD[(Long, (Double, Option[Double]))] = grid0.leftOuterJoin(averagedValues) //averagedValues, stdevduration\n",
    "    // result_2.take(20).foreach(println)\n",
    "\n",
    "    //TODO: 1. If value in original .tiff  , but None in seasonality ==> set NaN for the corresponding index\n",
    "var result: RDD[(Long, Double)] = result_2.map { \n",
    "    case (index, (value_org, value_pheno)) => {if (value_pheno.isDefined)  (index,value_pheno.get)\n",
    "                                               else (index, Double.NaN)  }\n",
    "}\n",
    "\n",
    "\n",
    "var season = DoubleArrayTile(result.sortBy(_._1).map(_._2).collect(),num_cols_rows._1, num_cols_rows._2)\n",
    "\n",
    "var geoTif = new SinglebandGeoTiff(season, projected_extent.extent, projected_extent.crs, Tags.empty, GeoTiffOptions(compression.DeflateCompression))\n",
    "var output = \"/data/local/home/parrot/minio2/\" + \"average_1\" + \".tif\"\n",
    "GeoTiffWriter.write(geoTif, output)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
