{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geotrellis.raster._\n",
    "import geotrellis.proj4.CRS\n",
    "import geotrellis.raster.io.geotiff.writer.GeoTiffWriter\n",
    "import geotrellis.raster.io.geotiff.{SinglebandGeoTiff, _}\n",
    "import geotrellis.raster.{CellType, DoubleArrayTile}\n",
    "import geotrellis.spark.io.hadoop._\n",
    "import geotrellis.vector.{Extent, ProjectedExtent}\n",
    "import org.apache.spark.mllib.linalg.Vector\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.input.PortableDataStream\n",
    "import org.apache.spark.sql._\n",
    "import java.time.{ZonedDateTime, LocalDateTime}\n",
    "import java.time.format.DateTimeFormatter\n",
    "import java.sql.Date\n",
    "import org.apache.spark.sql.types.{DateType, IntegerType}\n",
    "import java.util.Calendar\n",
    "import java.time.temporal.ChronoUnit\n",
    "import java.util.GregorianCalendar\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression.LinearRegressionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.ml.util\n",
    "import org.apache.spark.ml.linalg.VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Ground Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///data/local/home/parrot/minio2/individual_phenometrics_data.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "//filter only the phenology markers indicating SOS\n",
    "var df_filt = df.filter(  $\"First_Yes_Year\" > 1998 &&  $\"First_Yes_Year\" < 2018 &&  $\"Kingdom\" === \"Plantae\" && \n",
    "                        ($\"Phenophase_Description\".contains(\"First bloom\") ||\n",
    "                        $\"Phenophase_Description\".contains(\"Breaking leaf buds\") ||\n",
    "                        $\"Phenophase_Description\".contains(\"Flowers or flower buds\") ||\n",
    "                        $\"Phenophase_Description\".contains(\"First leaf (historic lilac/honeysuckle)\")))\n",
    "\n",
    "\n",
    "\n",
    "//select only the columns which we need for the ground comparison\n",
    "var pheno_df  = df_filt.select(\"Latitude\",\"Longitude\",\"Species\",\"Phenophase_Description\",\"First_Yes_Year\",\"First_Yes_Month\",\"First_Yes_Day\",\"First_Yes_Julian_Date\")\n",
    "\n",
    "\n",
    "\n",
    "pheno_df.groupBy(\"Species\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "//convert types\n",
    "var pheno_rdd: RDD[(Double, Double,String, String, Integer, Integer,Integer, Double)] = pheno_df.rdd.map( r => (r.getString(0).toDouble, r.getString(1).toDouble, //lat.long\n",
    "                                                                              \n",
    "                                                                                                         r.getString(2),r.getString(3),  //Pheno_description\n",
    "                                                                                r.getString(4).toInt, r.getString(5).toInt, r.getString(6).toInt,r.getString(7).toDouble))\n",
    "\n",
    "\n",
    "var speciesType = \"agrifolia\"\n",
    "var outputresults = \"Evi_1_\" + speciesType + \".csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Extent of SOS Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Single band GeoTiff\n",
    "var filepath = \"hdfs:///user/hadoop/evi_new_template.tif\"\n",
    "\n",
    "//Since it is a single GeoTiff, it will be a RDD with a tile.\n",
    "var geoTiff : RDD[(ProjectedExtent, Tile)] = sc.hadoopGeoTiffRDD(filepath)\n",
    "var proExtents_RDD : RDD[ProjectedExtent] = geoTiff.keys\n",
    "var extents_withIndex = proExtents_RDD.zipWithIndex().map{case (e,v) => (v,e)}\n",
    "var projected_extent :ProjectedExtent = (extents_withIndex.filter(m => m._1 == 0).values.collect())(0)\n",
    "\n",
    "var rasterExtent = RasterExtent(projected_extent.extent, 6500, 3000) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract TimeSat SOS Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pattern: String = \"tif\"\n",
    "var filepath: String = \"hdfs:///user/hadoop/NDVI_experiment_7_SOS\"  \n",
    "\n",
    "//give the years; HadoopGeoTiffRDD will preserve the file order from HDFS\n",
    "val tiles_RDD = sc.hadoopGeoTiffRDD(filepath, pattern).values.zipWithIndex.map{case (v,e) => (e+1999,v)}\n",
    "\n",
    "\n",
    "//year,array of values\n",
    "var grids_RDD: RDD[(Long,Array[Double])] = tiles_RDD.map(m => ( m._1, m._2.toArrayDouble()))\n",
    "\n",
    "var indexedValues: RDD[(Long,Array[(Int, Double)])] =  grids_RDD.map(m => (m._1, m._2.zipWithIndex.map{case (v,e) => (e,v)}))\n",
    "\n",
    "\n",
    "\n",
    "var rdd_year_and_tuple : RDD[(Long, (Int, Double))]  = indexedValues.flatMap{case (year, array_list) => array_list.map(tuple => (year, tuple))}.filter( m => !m._2._2.isNaN && m._2._2 >0)\n",
    "\n",
    "var time_sat_output =   rdd_year_and_tuple.map{case(year,(index,value)) => ((year.toInt,index.toLong),value)}\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Grid Coordinates For the Ground Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGridCoordinates(lat:Double, long: Double, species: String, pheno_type: String, year: Integer, month: Integer, day:Integer, julianYear : Double): ((Int,Long), (Double,String))={\n",
    "\n",
    "    var (col, row) = rasterExtent.mapToGrid(long, lat)\n",
    "    \n",
    "    var date_observations : Calendar = Calendar.getInstance()\n",
    "    date_observations.set(year.toInt ,month.toInt-1 , day.toInt) \n",
    "    var day_of_year : Double = date_observations.get(Calendar.DAY_OF_YEAR)\n",
    "    //cols * row + col\n",
    "    var index =  (6500.toInt*(row.toInt) + (col.toInt)).toLong\n",
    "    \n",
    "    return ((year, index),(day_of_year,species))\n",
    "}\n",
    "\n",
    "\n",
    "var ground_observartions = pheno_rdd.map(m =>  getGridCoordinates(m._1, m._2, m._3,m._4,m._5,m._6, m._7,m._8)).filter( m => m._2._1 >=0 && m._2._1 <270)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Unique Observations (Diffrent Years and diffrent sites for a year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var joined= time_sat_output.join(ground_observartions).filter(m => (m._2._2._2 == speciesType))\n",
    "var grouped_by_year_pixel = joined.groupByKey().map(rec => (rec._1, rec._2.toList))\n",
    "var minvalues = grouped_by_year_pixel.map(m =>(m._1, m._2.min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//drop the index and year, get TimeSat value, phenovalue,species\n",
    "var compare   :RDD[(Double, Double)]= minvalues.map(m => (m._2._2._1, m._2._1))\n",
    "\n",
    "//here filter on species,flatter the (timeSat,ground) values, make it to List of type (timeSatValue, Vector.danse(ground_value))\n",
    "var oneSpecies = compare.map(m => (m._1, Vectors.dense(m._2)))\n",
    "\n",
    "//RDD used for printing\n",
    "var oneSpeciestoPrint = compare.map(m => (m._1, m._2))\n",
    "oneSpeciestoPrint.take(10000).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var abs = oneSpeciestoPrint.map(m => Math.abs(m._1 - m._2))\n",
    "var total = abs.count\n",
    "var reduce = abs.reduce((_ + _))\n",
    "\n",
    "print (reduce/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit SOS Observations and Predictions into a Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val df2 = spark.createDataFrame(oneSpecies).toDF(\"label\",\"features\")\n",
    "val df3 = spark.createDataFrame(oneSpeciestoPrint).toDF(\"label\",\"features\")\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.3)\n",
    "  .setElasticNetParam(0.8)\n",
    "val lrModel : LinearRegressionModel = lr.fit(df2)\n",
    "\n",
    "\n",
    "val trainingSummary = lrModel.summary\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "println(s\"Mean Absolute error: ${trainingSummary.meanAbsoluteError}\")\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(\",\")}]\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
